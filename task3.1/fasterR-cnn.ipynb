{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23543a00",
   "metadata": {},
   "source": [
    "# Faster R-CNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59faed2",
   "metadata": {},
   "source": [
    "## Transforms / Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72025cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms._presets.ObjectDetection'>\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np, os, torch, random, cv2, json\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "num_classes = 12\n",
    "weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2( weights=weights)\n",
    "# Replace the classifier with a new one for your number of classes\n",
    "transforms1 = weights.transforms\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(transforms1)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "data_aug = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0), ratio=(0.95, 1.05)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_in = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914e344",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831e9ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: 1442\n",
      "image count 0\n",
      "image count 100\n",
      "image count 200\n",
      "image count 300\n",
      "image count 400\n",
      "image count 500\n",
      "image count 600\n",
      "image count 700\n",
      "image count 800\n",
      "image count 900\n",
      "image count 1000\n",
      "image count 1100\n",
      "image count 1200\n",
      "image count 1300\n",
      "image count 1400\n",
      "Number of valid images: 330\n",
      "image count 0\n",
      "image count 100\n",
      "image count 200\n",
      "image count 300\n",
      "Number of test images: 306\n",
      "image count 0\n",
      "image count 100\n",
      "image count 200\n",
      "image count 300\n"
     ]
    }
   ],
   "source": [
    "def convert_bbox_format(bbox):\n",
    "    \"\"\"\n",
    "    Convert [x, y, width, height] to [x_min, y_min, x_max, y_max]\n",
    "    \n",
    "    Args:\n",
    "        bbox (list): [x_center, y_center, width, height]\n",
    "    \n",
    "    Returns:\n",
    "        list: [x_min, y_min, x_max, y_max]\n",
    "    \"\"\"\n",
    "    x, y, w, h = bbox\n",
    "    x_min = x\n",
    "    y_min = y\n",
    "    x_max = x + w\n",
    "    y_max = y + h\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, root_dir, partition, transform=None):\n",
    "        self.anns = json.load(open(os.path.join(root_dir, 'annotations.json')))\n",
    "        self.categories = [c['name'] for c in self.anns['categories']]\n",
    "        self.root = root_dir\n",
    "        self.ids = []\n",
    "        self.file_names = []\n",
    "        for x in self.anns['images']:\n",
    "            self.file_names.append(x['path'])\n",
    "            self.ids.append(x['id'])\n",
    "        self.file_names = np.asarray(self.file_names)\n",
    "        self.ids = np.asarray(self.ids)\n",
    "        # create a list of size num_images, each element is a list of pieces\n",
    "        self.boardLabels = [[] for _ in range(len(self.ids))]\n",
    "        self.boardBB = [[] for _ in range(len(self.ids))]\n",
    "        for piece in self.anns['annotations']['pieces']:\n",
    "            idx = np.where(self.ids == piece['image_id'])[0][0]\n",
    "            if \"bbox\" in piece.keys():\n",
    "                bbox = convert_bbox_format(piece['bbox'])\n",
    "                self.boardBB[idx].append(bbox)\n",
    "                self.boardLabels[idx].append(piece['category_id'])\n",
    "        if partition == 'train':\n",
    "            self.split_ids = np.asarray(self.anns['splits'][\"chessred2k\"]['train']['image_ids']).astype(int)\n",
    "        elif partition == 'valid':\n",
    "            self.split_ids = np.asarray(self.anns['splits'][\"chessred2k\"]['val']['image_ids']).astype(int)\n",
    "        else:\n",
    "            self.split_ids = np.asarray(self.anns['splits'][\"chessred2k\"]['test']['image_ids']).astype(int)\n",
    "        self.split_ids = self.split_ids#[:5]\n",
    "        intersect = np.isin(self.ids, self.split_ids)\n",
    "        self.split_ids = np.where(intersect)[0]\n",
    "        self.file_names = [self.file_names[i] for i in self.split_ids]\n",
    "        self.boardBB = [self.boardBB[i] for i in self.split_ids]\n",
    "        self.boardLabels = [self.boardLabels[i] for i in self.split_ids]\n",
    "        #self.num_pieces = F.one_hot(self.num_pieces.long()-1, 32)\n",
    "        self.ids = self.ids[self.split_ids]\n",
    "\n",
    "        self.transform = transform\n",
    "        print(f\"Number of {partition} images: {len(self.file_names)}\")\n",
    "        self.images = {}\n",
    "        counter = 0\n",
    "        for i in range(len(self.file_names)):\n",
    "            #image = cv2.imread(os.path.join(self.root, self.file_names[i]))\n",
    "            #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.open(os.path.join(self.root, self.file_names[i]))\n",
    "            if counter%100==0:\n",
    "                print(\"image count\", counter)\n",
    "            counter += 1\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            self.images[self.file_names[i]] = image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image = self.images[self.file_names[i]]\n",
    "        boxes = self.boardBB[i]\n",
    "        labels = self.boardLabels[i]\n",
    "        boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        area = (boxes_tensor[:, 3] - boxes_tensor[:, 1]) * (boxes_tensor[:, 2] - boxes_tensor[:, 0])\n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"]= torch.tensor([i])  # Required for evaluation\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        # format the target to handle the Dataloader\n",
    "        #target =\n",
    "        return image,target\n",
    "\n",
    "train_dataset = ChessDataset('..', 'train', data_aug)\n",
    "valid_dataset = ChessDataset('..', 'valid', data_in)\n",
    "test_dataset = ChessDataset('..', 'test', data_in)\n",
    "\n",
    "batchsize = 3\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for object detection batches.\n",
    "    Handles variable numbers of bounding boxes per image.\n",
    "    \"\"\"\n",
    "    images = {}\n",
    "    targets = []\n",
    "    \n",
    "    for img, target in batch:\n",
    "        images[target['image_id']] = img\n",
    "        processed_target = {\n",
    "            'boxes': torch.as_tensor(target['boxes'], dtype=torch.float32),\n",
    "            'labels': torch.as_tensor(target['labels'], dtype=torch.int64),\n",
    "            'image_id': torch.as_tensor(target['image_id'], dtype=torch.int64),\n",
    "            'area': torch.as_tensor(target['area'], dtype=torch.float32),\n",
    "            'iscrowd': torch.as_tensor(target['iscrowd'], dtype=torch.int64)\n",
    "        }\n",
    "        targets.append(processed_target)\n",
    "    \n",
    "    # Stack images (they should all be the same size after transforms)\n",
    "    images = torch.stack(list(images.values()), dim=0)\n",
    "    \n",
    "    return images, targets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, num_workers=0,collate_fn=collate_fn)#,collate_fn=collate_fn\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False, num_workers=0,collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False, num_workers=0,collate_fn=collate_fn )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72efba",
   "metadata": {},
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f40edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_iter(dataloader, model, loss_fn, optimizer=None, is_train=True):\n",
    "    if is_train:\n",
    "      assert optimizer is not None, \"When training, please provide an optimizer.\"\n",
    "      \n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    #if is_train:\n",
    "    # so that the output is always the loss function easier to know the loss\n",
    "    model.train() # put model in train mode\n",
    "    #else:\n",
    "    #  model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    metrics = {'loss': 0, 'class_loss': 0, 'box_loss': 0, 'rpn_loss': 0}\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "      for batch,(images, targets) in enumerate(tqdm(dataloader)):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        if is_train:\n",
    "          optimizer.zero_grad()\n",
    "          losses = sum(loss for loss in loss_dict.values())\n",
    "          # Backpropagation\n",
    "          \n",
    "          losses.backward()\n",
    "          optimizer.step()\n",
    "        else:\n",
    "          losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        metrics['loss'] += losses.item()\n",
    "        metrics['class_loss'] += loss_dict['loss_classifier'].item()\n",
    "        metrics['box_loss'] += loss_dict['loss_box_reg'].item()\n",
    "        metrics['rpn_loss'] += loss_dict['loss_objectness'].item() + loss_dict['loss_rpn_box_reg'].item()\n",
    "    return metrics['loss'] / num_batches, metrics['class_loss']/ num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e32078f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer):\n",
    "  train_history = {'loss': [], 'accuracy': []}\n",
    "  val_history = {'loss': [], 'accuracy': []}\n",
    "  best_val_loss = np.inf\n",
    "  print(\"Start training...\")\n",
    "  for t in range(num_epochs):\n",
    "      print(f\"\\nEpoch {t+1}\")\n",
    "      train_loss, train_acc = epoch_iter(train_dataloader, model, loss_fn, optimizer)\n",
    "      print(f\"Train loss: {train_loss:.3f} \\t Train metric: {train_acc:.3f}\")\n",
    "      val_loss, val_acc = epoch_iter(validation_dataloader, model, loss_fn, is_train=False,optimizer=optimizer)\n",
    "      print(f\"Val loss: {val_loss:.3f} \\t Val metric: {val_acc:.3f}\")\n",
    "\n",
    "      # save model when val loss improves\n",
    "      if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
    "        torch.save(save_dict, model_name + '_best_model.pth')\n",
    "\n",
    "      # save latest model\n",
    "      save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
    "      torch.save(save_dict, model_name + '_latest_model.pth')\n",
    "\n",
    "      # save training history for plotting purposes\n",
    "      train_history[\"loss\"].append(train_loss)\n",
    "      train_history[\"accuracy\"].append(train_acc)\n",
    "\n",
    "      val_history[\"loss\"].append(val_loss)\n",
    "      val_history[\"accuracy\"].append(val_acc)\n",
    "      \n",
    "  print(\"Finished\")\n",
    "  return train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd03f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 481/481 [02:58<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 111.114 \t Train metric: 1198.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:16<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 94.600 \t Val metric: 234.912\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 481/481 [02:53<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 104.172 \t Train metric: 1031.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:15<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 91.622 \t Val metric: 235.758\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 481/481 [02:53<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 103.339 \t Train metric: 1032.389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:16<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 98.400 \t Val metric: 235.259\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 408/481 [02:27<00:26,  2.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# TODO - Train the model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 11\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m train(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchess_model\u001b[39m\u001b[38;5;124m'\u001b[39m, num_epochs, train_dataloader, valid_dataloader, loss_fn, optimizer)\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m epoch_iter(train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Train metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m epoch_iter(validation_dataloader, model, loss_fn, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m, in \u001b[0;36mepoch_iter\u001b[1;34m(dataloader, model, loss_fn, optimizer, is_train)\u001b[0m\n\u001b[0;32m     30\u001b[0m   losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Accumulate metrics\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     34\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     35\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_box_reg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define loss function\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.MSELoss()\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "num_epochs = 50\n",
    "\n",
    "# TODO - Train the model\n",
    "model.to(device)\n",
    "train_history, val_history = train(model, 'chess_model', num_epochs, train_dataloader, valid_dataloader, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add9e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating IoU: 100%|██████████| 2/2 [00:00<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average IoU: 0.0000\n",
      "Matched Ratio: 0.00% (percentage of predictions that matched ground truth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.ops import box_iou\n",
    "def calculate_average_iou(dataloader, model, device, score_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate average IoU between predictions and ground truth boxes\n",
    "    \n",
    "    Args:\n",
    "        dataloader: PyTorch DataLoader yielding (images, targets)\n",
    "        model: Your detection model\n",
    "        device: torch.device\n",
    "        score_threshold: Minimum confidence score to consider a prediction\n",
    "        \n",
    "    Returns:\n",
    "        mean_iou: Average IoU across all matched predictions\n",
    "        matched_ratio: Percentage of predictions that matched with ground truth\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_iou = 0.0\n",
    "    total_matches = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(dataloader, desc=\"Calculating IoU\"):\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            predictions = model(images)\n",
    "            \n",
    "            for pred, target in zip(predictions, targets):\n",
    "                # Filter predictions by score threshold\n",
    "                keep = pred['scores'] >= score_threshold\n",
    "                pred_boxes = pred['boxes'][keep]\n",
    "                gt_boxes = target['boxes']\n",
    "                \n",
    "                if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate IoU matrix\n",
    "                iou_matrix = box_iou(pred_boxes, gt_boxes)\n",
    "                \n",
    "                # Find best matches (one prediction can only match one ground truth)\n",
    "                pred_matches = iou_matrix.argmax(dim=1)  # For each pred, find best gt\n",
    "                best_ious = iou_matrix[torch.arange(len(pred_matches)), pred_matches]\n",
    "                \n",
    "                # Filter matches where IoU > 0\n",
    "                valid_matches = best_ious > 0\n",
    "                total_iou += best_ious[valid_matches].sum().item()\n",
    "                total_matches += valid_matches.sum().item()\n",
    "                total_predictions += len(pred_boxes)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mean_iou = total_iou / total_matches if total_matches > 0 else 0.0\n",
    "    matched_ratio = total_matches / total_predictions if total_predictions > 0 else 0.0\n",
    "    \n",
    "    return mean_iou, matched_ratio\n",
    "mean_iou, matched_ratio = calculate_average_iou(\n",
    "    dataloader=test_dataloader,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    score_threshold=0.5  # Only consider predictions with confidence > 50%\n",
    ")\n",
    "\n",
    "print(f\"Average IoU: {mean_iou:.4f}\")\n",
    "print(f\"Matched Ratio: {matched_ratio:.2%} (percentage of predictions that matched ground truth)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
